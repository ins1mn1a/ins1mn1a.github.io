<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Kafka | ins1mn1a</title><meta name="keywords" content="Kafka"><meta name="author" content="ins1mnia"><meta name="copyright" content="ins1mnia"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Kafka，基本概念，生产者原理，消费者原理，消费者组原理，分区，副本，集群搭建，常见消息队列问题比如消息重复">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka">
<meta property="og:url" content="http://ins1mn1a.github.io/2022/07/27/Kafka/Kafka/index.html">
<meta property="og:site_name" content="ins1mn1a">
<meta property="og:description" content="Kafka，基本概念，生产者原理，消费者原理，消费者组原理，分区，副本，集群搭建，常见消息队列问题比如消息重复">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://ins1mn1a.github.io/img/background_img/1/6.jpg">
<meta property="article:published_time" content="2022-07-27T14:46:17.266Z">
<meta property="article:modified_time" content="2022-07-27T14:55:47.698Z">
<meta property="article:author" content="ins1mnia">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ins1mn1a.github.io/img/background_img/1/6.jpg"><link rel="shortcut icon" href="/img/background_img/%E7%8E%89%E5%AD%90.icon.png"><link rel="canonical" href="http://ins1mn1a.github.io/2022/07/27/Kafka/Kafka/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Kafka',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-27 22:55:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ins1mn1a" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://tvax1.sinaimg.cn/crop.0.0.996.996.180/007RWaeLly8gr7p82e9e6j30ro0rognd.jpg?KID=imgbed,tva&amp;Expires=1639625304&amp;ssig=yn%2FY%2BCtxLO" onerror="onerror=null;src='/img/background_img/error_page.jpeg'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background_img/1/6.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ins1mn1a</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Kafka</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-27T14:46:17.266Z" title="发表于 2022-07-27 22:46:17">2022-07-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-27T14:55:47.698Z" title="更新于 2022-07-27 22:55:47">2022-07-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Kafka/">Kafka</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>46分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Kafka"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/07/27/Kafka/Kafka/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2022/07/27/Kafka/Kafka/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Kafka介绍"><a href="#Kafka介绍" class="headerlink" title="Kafka介绍"></a>Kafka介绍</h2><p>Kafka是最初由Linkedin公司开发，是一个<strong>分布式</strong>、<strong>支持分区</strong>的（<strong>partition</strong>）、<strong>多副本</strong>的 （<strong>replica</strong>），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理 大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、 Storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于 2010 年贡献给了Apache基金会并成为顶级开源项目。</p>
<h3 id="Kafka使用场景"><a href="#Kafka使用场景" class="headerlink" title="Kafka使用场景"></a>Kafka使用场景</h3><p><strong>日志收集：</strong>可以用kafka收集各种服务的日志，通过Kafka以统一接口服务的方式 开放给各种consumer，比如 Hadoop，Hbase，Solr等。</p>
<p><strong>消息系统</strong>：Kafka可以同消息队列一样，用于解耦，不用关系消息怎么消费，只需要放到队列里去就可以了。</p>
<p><strong>用户活动跟踪（埋点）</strong>：当用户在浏览网页的同时，可以记录用户的行为，比如浏览网页，点赞，搜索等行为，这些信息可以发布到Kafka的各个Topic中，然后订阅者 订阅topic去做对应监控分析，或者到Hadoop等。</p>
<h3 id="Kafka基本概念"><a href="#Kafka基本概念" class="headerlink" title="Kafka基本概念"></a>Kafka基本概念</h3><p><strong>Broker：</strong>一个Broker也就是一个Kafka，多个Broker可以构建成Kafka集群</p>
<p><strong>Producer：</strong>消息生产者，向Broker发送消息的客户端</p>
<p><strong>Consumer：</strong>消息消费者，从Broker读取数据的客户端</p>
<p><strong>Topic：</strong>topic是消息的分类，可以有多个topic，Producer发送的每条消息都必须要有一个topic，可以理解为队列</p>
<p><strong>ConsumerGroup：</strong>消费者组，每一个Consumer属于一个ConsumerGroup，<strong>一条消息可以被不同的ConsumerGroup消费，但是一个ConsumerGroup中只能有一个Consumer消费该消息</strong></p>
<p><strong>Partition</strong>：分区，每个topic可以分为多个partition</p>
<p><strong>Leader：</strong>每个分区都会有 1 个leader，leader负责处理读写操作</p>
<p><strong>Follower：</strong>每个分区除了 1 个leader外，还有多个 follower 副本，follower只用于leader发生故障，follower会被重新选举为leader</p>
<h3 id="Kafka架构图"><a href="#Kafka架构图" class="headerlink" title="Kafka架构图"></a>Kafka架构图</h3><p>下面是Kafka架构图</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220705110316426.png" alt="image-20220705110316426"></p>
<h2 id="Kafka基本使用"><a href="#Kafka基本使用" class="headerlink" title="Kafka基本使用"></a>Kafka基本使用</h2><p>在安装Kafka之前，需要安装JDK，Zookeeper，Kafka在2.8版本以后就可以不使用Zookeeper了</p>
<h3 id="Kafka搭建"><a href="#Kafka搭建" class="headerlink" title="Kafka搭建"></a>Kafka搭建</h3><p>已安装好ZK3.7.0，在阿里云的镜像上下载Kafka2.13_3.2.0，2.13代表Scala的版本2.13,3.2.0是kafka的版本</p>
<p>Kafka安装到/usr/local/kafka的目录下，然后修改kafka的配置文件。</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#broker.id属性在kafka集群中必须要是唯一</span></span><br><span class="line"><span class="meta">broker.id</span>= <span class="string">0</span></span><br><span class="line"><span class="comment">#kafka部署的机器ip和提供服务的端口号</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.73.138:9092</span></span><br><span class="line"><span class="comment">#kafka的消息存储文件</span></span><br><span class="line"><span class="meta">log.dir</span>=<span class="string">/usr/local/data/kafka-logs</span></span><br><span class="line"><span class="comment">#kafka连接zookeeper的地址</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>= <span class="string">192.168.73.137:2181</span></span><br></pre></td></tr></table></figure>

<p>先启动Zookeeper，然后启动Kafka</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-server-start.sh -daemon ../config/server.properties</span><br></pre></td></tr></table></figure>

<p>然后连接到Zookeeper客户端  ./zkCli.sh -server 192.168.73.137:2181</p>
<p>查看 /brokers/ids目录下有不有 broker.id=0 的broker</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220704231503309.png" alt="image-20220704231503309"></p>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>server.properties 配置文件：</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>broker.id</td>
<td>0</td>
<td>每个broker都可以⽤⼀个唯⼀的⾮负整数id进⾏标识；</td>
</tr>
<tr>
<td>log.dirs</td>
<td>/tmp/kafka-logs</td>
<td>kafka存放数据的路径。这个路径并不是唯⼀的，可以是多个，路径之间只需要使⽤逗号分隔即可；每当创建新partition时，都会选择在包含最少partitions的路径下进行。</td>
</tr>
<tr>
<td>listeners</td>
<td>PLAINTEXT://192.168.73.137:9092</td>
<td>server接受客户端连接的端⼝，ip配置kafka本机ip即可</td>
</tr>
<tr>
<td>zookeeper.connect</td>
<td>localhost:2181</td>
<td>zooKeeper连接字符串的格式为：hostname:port，此处hostname和port分别是ZooKeeper集群中某个节点的host和port；zookeeper如果是集群，连接⽅式为hostname1:port1, hostname2:port2,hostname3:port3</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>168</td>
<td>每个⽇志⽂件删除之前保存的时间。默认数据保存时间对所有topic都⼀样。</td>
</tr>
<tr>
<td>num.partitions</td>
<td>1</td>
<td>创建topic的默认分区数</td>
</tr>
<tr>
<td>default.replication.factor</td>
<td>1</td>
<td>⾃动创建topic的默认副本数量，建议设置为⼤于等于2</td>
</tr>
<tr>
<td>min.insync.replicas</td>
<td>1</td>
<td>当producer设置acks为-1时，min.insync.replicas指定replicas的最⼩数⽬（必须确认每⼀个repica的写数据都是成功的），如果这个数⽬没有达到，producer发送消息会产⽣异常</td>
</tr>
<tr>
<td>delete.topic.enable</td>
<td>false</td>
<td>是否允许删除主题</td>
</tr>
</tbody></table>
<h3 id="创建-topic"><a href="#创建-topic" class="headerlink" title="创建 topic"></a>创建 topic</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在3.2版本使用这个命令会报错 zookeeper is not a recogined option，新版Kafka可以脱离zookeeper了，kafka版本太高了</span></span><br><span class="line">./kafka-topics.sh --create --zookeeper 172.16.253.35:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure>

<p>创建主题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 单机Kafka可以不用指定分区</span></span><br><span class="line">./kafka-topics.sh --create --bootstrap-server 192.168.73.137:9092 --topic first_test_topic --partition 1 --replication-factor 1</span><br><span class="line"><span class="comment"># 查询所有主题</span></span><br><span class="line">./kafka-topics.sh --bootstrap-server 192.168.73.137:9092 --list</span><br><span class="line"><span class="comment"># 查询Kafka的指定主题</span></span><br><span class="line">./kafka-topics.sh --bootstrap-server 192.168.73.137:9092 --describe first_test_topic</span><br></pre></td></tr></table></figure>

<h3 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@192 bin]<span class="comment"># ./kafka-console-producer.sh --bootstrap-server 192.168.73.137:9092 --topic first_test_topic</span></span><br><span class="line">OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, <span class="keyword">then</span> you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N</span><br><span class="line">&gt;hello</span><br></pre></td></tr></table></figure>

<p><img src="/../../img/markdown_img/kafka.assets/image-20220705114403700.png" alt="image-20220705114403700"></p>
<h3 id="消费消息"><a href="#消费消息" class="headerlink" title="消费消息"></a>消费消息</h3><p>这个时候，我们上面生产者已经生产了一条 hello 消息，这个时候我们再去消费</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 默认是消费最新的消息，是从历史消息中最后一条消息的偏移量+1开始消费的，也就是这个Consumer启动后，接收到的第一条消息</span></span><br><span class="line"><span class="comment"># 因此，上面的 hello 是消费不到的</span></span><br><span class="line">[root@192 bin]<span class="comment"># ./kafka-console-consumer.sh --bootstrap-server 192.168.73.137:9092 --topic first_test_topic</span></span><br><span class="line">OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, <span class="keyword">then</span> you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N</span><br></pre></td></tr></table></figure>

<p><img src="/../../img/markdown_img/kafka.assets/image-20220705114717746.png" alt="image-20220705114717746"></p>
<p>这时候，生产者再发送一条消息，消费者这里就可以消费到了</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220705114753416.png" alt="image-20220705114753416"></p>
<p>还有一种模式，可以直接 <strong>从头开始 消费</strong>，这样，历史的消息记录，也可以消费得到</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@192 bin]<span class="comment"># ./kafka-console-consumer.sh --bootstrap-server 192.168.73.137:9092 --topic first_test_topic --from-beginning</span></span><br><span class="line">OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, <span class="keyword">then</span> you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N</span><br><span class="line">hello</span><br><span class="line">kafka</span><br></pre></td></tr></table></figure>

<p><img src="/../../img/markdown_img/kafka.assets/image-20220705114824630.png" alt="image-20220705114824630"></p>
<h3 id="单播消息"><a href="#单播消息" class="headerlink" title="单播消息"></a>单播消息</h3><p>单播消息：一个消费组里 只会有一个消费者能消费到某一个topic中的消息。于是可以创建多个消费者，这些消费者在同一个消费组中。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server 192.168.73.137:9092 --consumer-property group.id=testGroup --topic first_test_topic</span><br></pre></td></tr></table></figure>

<h3 id="多播消息"><a href="#多播消息" class="headerlink" title="多播消息"></a>多播消息</h3><p>在一些业务场景中需要让一条消息被多个消费者消费，那么就可以使用多播模式。kafka实现多播，只需要让不同的消费者处于不同的消费组即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server 192.168.73.137:9092 --consumer-property group.id=testGroup1 --topic first_test_topic</span><br><span class="line"></span><br><span class="line">./kafka-console-consumer.sh --bootstrap-server 192.168.73.137:9092 --consumer-property group.id=testGroup2 --topic first_test_topic</span><br></pre></td></tr></table></figure>

<h3 id="查看消费组"><a href="#查看消费组" class="headerlink" title="查看消费组"></a>查看消费组</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看当前主题下有哪些消费组</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server 192.168.73.137:9092 --list</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看消费组中的具体信息：比如当前偏移量、最后一条消息的偏移量、堆积的消息数量</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server 192.168.73.137:9092 --describe --group testGroup</span><br></pre></td></tr></table></figure>

<h2 id="Kafka集群搭建"><a href="#Kafka集群搭建" class="headerlink" title="Kafka集群搭建"></a>Kafka集群搭建</h2><p>这里采用三台虚拟机</p>
<ol>
<li><p>先搭建好 Zookeeper 集群</p>
</li>
<li><p>进行配置文件目录，修改配置文件<code>server.properties</code></p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 每个broker的id是唯一的</span></span><br><span class="line">broker.id=0	</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定监听的地址和端口号，IP是内网IP</span></span><br><span class="line">listeners=PLAINTEXT://192.168.73.137:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定监听的地址和端口号，IP是外网IP</span></span><br><span class="line"><span class="meta">#</span><span class="bash">advertised.listeners=PLAINTEXT://your.host.name:9092</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定数据日志目录</span></span><br><span class="line">log.dirs=/usr/local/kafka/logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定zk的连接地址</span></span><br><span class="line">zookeeper.connect=192.168.73.137:2181,192.168.73.136:2181,192.168.73.135:2181</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>启动Kafka集群</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@192 kafka]# ./bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@192 kafka]# netstat -lntp | grep 9092</span><br><span class="line">tcp6       0      0 192.168.73.137:9092     :::*                    LISTEN      19603/java</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>到Zookeeper集群下查看所有的broker</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /brokers/ids</span><br><span class="line">[0, 1, 2]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] </span><br></pre></td></tr></table></figure>

<p>修改分区数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@192 bin]# ./kafka-topics.sh --bootstrap-server 192.168.73.137:9092 --alter --topic first_test_topic --partitions 3</span><br><span class="line">OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N</span><br><span class="line">[root@192 bin]# ./kafka-topics.sh --bootstrap-server 192.168.73.137:9092 --describe --topic first_test_topic</span><br><span class="line">OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N</span><br><span class="line">Topic: first_test_topic	TopicId: vi8l4UdNR8ydP9o0bhCYYA	PartitionCount: 3	ReplicationFactor: 1	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first_test_topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</span><br><span class="line">	Topic: first_test_topic	Partition: 1	Leader: 1	Replicas: 1	Isr: 1</span><br><span class="line">	Topic: first_test_topic	Partition: 2	Leader: 2	Replicas: 2	Isr: 2</span><br></pre></td></tr></table></figure>





<h2 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h2><h3 id="生产者原理"><a href="#生产者原理" class="headerlink" title="生产者原理"></a>生产者原理</h3><p>Kafka在消息发送的过程中，涉及到两个线程——<strong>main</strong> 线程和 <strong>sender</strong> 线程。在 main 线程中创建一个了<strong>缓冲区 记录累加器 RecordAccumulator</strong>，里面有多个双端队列，<strong>一个分区对应一个双端队列</strong>。main 线程将消息发送给 RecordAccumulator，sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。</p>
<ol>
<li>Kafka生产者将生产的消息封装成一个 <strong>ProducerRecord</strong> 向Kafka中某个topic发送消息</li>
<li>首先 发送的消息 会经过一个 <strong>拦截器Interceptor</strong>，可以对 消息进行一些额外的封装</li>
<li>发送的消息 首先要通过 <strong>序列化器</strong> 进行 序列化，以便在网络中传输</li>
<li>发送的消息 接下来通过 <strong>分区器</strong> 指定该消息应该到哪一个分区，一个分区对应一个双端队列，如果消息指定了分区，那就不用分区器划分了</li>
<li>分好区的消息并不是立即发送，而是放入了RecordAccumulator（默认大小是32M），然后多个消息会被分个一个<strong>批次（batch）</strong>，默认一个批次是 <strong>16K</strong>。</li>
<li>Sender 线程启动后会去缓冲区中获取可以发送的批次。只有数据累积到<strong>batch.size</strong>之后，Sender才会发送数据，默认就是 <strong>16K</strong>。如果数据迟迟未到达batch.size，Sender等待 <strong>linger.ms</strong> 时间，然后就会直接发送消息。<strong>默认 linger.ms 是 0ms</strong>，表示没有延迟，因此会立即发送。生产环境中这两个参数都需要调整。</li>
<li>Sender发送时，以分区节点作为Key，也就是 broker1，broker2为 key，请求为value进行发送，形成一个请求。</li>
<li>请求在从 Sender线程发往Kafka之前还会保存到 <strong>InFlightRequests</strong> 中，主要作用就是<strong>缓存已经发出去还是还没有收到应答的请求</strong>。</li>
<li>请求发送到某个 Broker中，如果第一个请求发送到Broker1，broker1没有即使应答，就允许继续发送到第二个请求，如果一直到<strong>第5个请求</strong>都<strong>没有应答</strong>，<strong>后续</strong>的请求也<strong>不允许再发送</strong>了。</li>
<li>在InFlightRequests 通过配置参数去限制的这个请求的最大次数，也就是每个连接（也就是客户端与 Node 之间的连接）最多缓存的请求数。这个配置参数为 <strong>max.in.flight.requests.per.connection</strong>，<strong>默认值为5</strong>，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response）。通过比较 双端队列的 size 与这个参数的大小来判断对应的节点中是否已经堆积了很多未响应的消息，如果真是如此，那么说明这个节点负载较大或网络连接有问题，再继续向其发送请求会增大请求超时的可能。</li>
<li>Kafka集群收到请求后会有一个响应级别<strong>acks</strong>，分别为<strong>0，1，all/-1</strong><ul>
<li><strong>0</strong>：生产者发送过来的数据，<strong>不需要等待数据落到Leader的磁盘上</strong>；采用这种配置，可能生产者发出的消息还在半路上，然后目的地 partition leader 的Broker就直接挂掉了，然后这个客户却还认为消息发送成功了，这就导致消息丢失。</li>
<li><strong>1</strong>：生产者发送过来的数据，<strong>Partition Leader 接收到消息并且写入磁盘</strong>了，就认为消息<strong>发送成功</strong>了，不管其他的follower是否同步这条消息。这也是 kafka 的默认设置。采用这种配置，很明显当 分区Leader 已经把消息落入磁盘，所在的 broker 挂掉，follower 又还没同步数据，就会造成这条消息的丢失。</li>
<li><strong>all/-1</strong>：生产者发送过来的数据，要等 <strong>partition leader 的 ISR 中所有跟 Leader 保持同步的 Follower 都把这条消息同步完后</strong>，才认为这条消息<strong>写入成功</strong>。</li>
<li>kafka集群应答之后，如果<strong>成功</strong>，进行<strong>数据的清理</strong>，如果<strong>失败</strong>，进行<strong>重试</strong>，<strong>默认重试次数是int的最大值</strong> </li>
</ul>
</li>
</ol>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220705145304880.png" alt="image-20220705145304880"></p>
<h3 id="生产者消息发送"><a href="#生产者消息发送" class="headerlink" title="生产者消息发送"></a>生产者消息发送</h3><h4 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h4><p>如果ProducerRecord的partition<strong>指定了某个分区</strong>，那么<strong>消息体存储到指定的分区</strong>里</p>
<p>如果ProducerRecord的partition（其实就是分区号），如果<strong>没有指定分区器</strong>，则使用<strong>默认分区器DefaultPartitioner</strong>来计算，然后根据ProducerRecord的key来计算</p>
<p>如果<strong>key为空</strong>，则以<strong>轮询的方式</strong>来给主题下的各个<strong>可用分区</strong>来<strong>发送消息</strong>（注意这里，是可用分区）。</p>
<p>如果<strong>key不为空</strong>，则根据<strong>hash算法</strong>（采用MurmurHash2算法，具备高运算性能及低碰撞率）<strong>计算分区号</strong>，<strong>相同的key会分配到同一个分区</strong>。</p>
<p>DefaultPartitioner：是默认的分区器</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706143920155.png" alt="image-20220706143920155"></p>
<ol>
<li>指明partition的情况下，直 接将指明的值作为partition值； 例如partition=0，所有数据写入 分区0<ol>
<li><strong>没有指明partition值</strong>但<strong>有key</strong>的情况下，将<strong>key的Murmur2hash值</strong>与<strong>topic的 partition数</strong>进行<strong>取余</strong>得到<strong>partition值</strong>； 例如：key1的Murmur2hash值=5， key2的Murmur2hash值=6 ，topic的partition数=2，那 么key1 对应的value1写入1号分区，key2对应的value2写入0号分区。</li>
</ol>
</li>
<li><strong>既没有partition值</strong> <strong>又没有key值</strong>的情况下，Kafka采用<strong>Sticky Partition（黏性分区器）</strong>，会随机选择一个分区，并尽可能一直 使用该分区，待该分区的batch已满或者已完成，Kafka再随机一个分区进行使用（和上一次的分区不同）。 例如：第一次随机选择0号分区，等0号分区当前批次满了（默认16k）或者linger.ms设置的时间到， Kafka再随机一个分区进行使用（如果还是0会继续随机）。</li>
</ol>
<p>还可以<strong>自定义分区</strong>：</p>
<p>定义类<strong>实现Partition接口</strong>，<strong>重写partition方法</strong>，然后用Properties <strong>注入到 ProducerConfig.PARTITIONER_CLASS_CONFIG</strong></p>
<h4 id="普通异步发送"><a href="#普通异步发送" class="headerlink" title="普通异步发送"></a>普通异步发送</h4><p>创建Kafka生产者，异步的方式发送到 Kafka Broker</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zc.kafkademo.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.protocol.types.Field;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 生产者普通异步发送</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// Kafka 生产者配置对象</span></span><br><span class="line">        Properties propertites = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// Kafka 配置对象添加配置信息</span></span><br><span class="line">        propertites.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value 序列化</span></span><br><span class="line">        propertites.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        propertites.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        <span class="comment">// 创建 Kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(propertites);</span><br><span class="line">        <span class="comment">// 调用 send 方法，发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first_test_topic&quot;</span>,<span class="string">&quot;ins1mnia message: &quot;</span>+i));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>先启动 kafka 消费者，然后IDEA运行</p>
<p>可以看到消费者端消费到消息</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220705163134082.png" alt="image-20220705163134082"></p>
<h4 id="带回调函数的异步发送"><a href="#带回调函数的异步发送" class="headerlink" title="带回调函数的异步发送"></a>带回调函数的异步发送</h4><p>回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 带有回调函数的异步发送</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallBack</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// kafka topic</span></span><br><span class="line">        <span class="keyword">final</span> String topic = <span class="string">&quot;first_test_topic&quot;</span>;</span><br><span class="line">        <span class="comment">// 创建生产者配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 配置 server</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 配置 序列化</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 创建kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line">        <span class="comment">// 发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 添加回调函数</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(topic,<span class="string">&quot;ins1mnia message: &quot;</span>+i),((metadata, exception) -&gt; &#123;</span><br><span class="line">                <span class="comment">// 该方法在Producer收到 ack 时调用，为异步调用</span></span><br><span class="line">                <span class="keyword">if</span> (exception==<span class="keyword">null</span>)</span><br><span class="line">                    System.out.println(<span class="string">&quot;topic：&quot;</span>+metadata.topic()+<span class="string">&quot;-&gt;&quot;</span>+<span class="string">&quot; partition：&quot;</span>+metadata.partition());</span><br><span class="line">                <span class="keyword">else</span> System.out.println(exception.getMessage());</span><br><span class="line">            &#125;));</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>先启动Kafka消费者，然后IDEA运行</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706101246974.png" alt="image-20220706101246974"></p>
<p>回调函数打印：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">topic：first_test_topic-&gt; partition：<span class="number">2</span></span><br><span class="line">topic：first_test_topic-&gt; partition：<span class="number">2</span></span><br><span class="line">topic：first_test_topic-&gt; partition：<span class="number">2</span></span><br><span class="line">topic：first_test_topic-&gt; partition：<span class="number">2</span></span><br><span class="line"><span class="number">10</span>:<span class="number">10</span>:<span class="number">50.702</span> [kafka-producer-network-thread | producer-<span class="number">1</span>] DEBUG org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-<span class="number">1</span>] Received PRODUCE response from node <span class="number">1</span> <span class="function"><span class="keyword">for</span> request with header <span class="title">RequestHeader</span><span class="params">(apiKey=PRODUCE, apiVersion=<span class="number">9</span>, clientId=producer-<span class="number">1</span>, correlationId=<span class="number">6</span>)</span>: <span class="title">ProduceResponseData</span><span class="params">(responses=[TopicProduceResponse(name=<span class="string">&#x27;first_test_topic&#x27;</span>, partitionResponses=[PartitionProduceResponse(index=<span class="number">1</span>, errorCode=<span class="number">0</span>, baseOffset=<span class="number">0</span>, logAppendTimeMs=-<span class="number">1</span>, logStartOffset=<span class="number">0</span>, recordErrors=[], errorMessage=<span class="keyword">null</span>)</span>])], throttleTimeMs</span>=<span class="number">0</span>)</span><br><span class="line"><span class="number">10</span>:<span class="number">10</span>:<span class="number">50.702</span> [kafka-producer-network-thread | producer-<span class="number">1</span>] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-<span class="number">1</span>] ProducerId: <span class="number">1000</span>; Set last ack<span class="string">&#x27;d sequence number for topic-partition first_test_topic-1 to 0</span></span><br><span class="line"><span class="string">topic：first_test_topic-&gt; partition：1</span></span><br></pre></td></tr></table></figure>



<h4 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h4><p>同步发送就是消息发送完毕后，还要等待结果的响应，然后才会发送下一条消息。producer的send方法返回一个Future对象，我们可以通过调用这个对象的get方法，阻塞当前线程，直到收到收到ack消息</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerSync</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// kafka topic</span></span><br><span class="line">        <span class="keyword">final</span> String topic = <span class="string">&quot;first_test_topic&quot;</span>;</span><br><span class="line">        <span class="comment">// 创建生产者配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 配置 server</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 配置 序列化</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 创建kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line">        <span class="comment">// 发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 异步发送 默认</span></span><br><span class="line">            <span class="comment">// kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first_test_topic&quot;,&quot;ins1mnia message: &quot;+i));</span></span><br><span class="line">            <span class="comment">// 同步发送</span></span><br><span class="line">            RecordMetadata recordMetadata = kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(topic, <span class="string">&quot;ins1mnia message: &quot;</span> + i)).get();</span><br><span class="line">            <span class="keyword">if</span> (recordMetadata!=<span class="keyword">null</span> &amp;&amp; recordMetadata.hasOffset())</span><br><span class="line">            &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;消息发送成功 -&gt;&quot;</span>+recordMetadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Kafka-消费者"><a href="#Kafka-消费者" class="headerlink" title="Kafka 消费者"></a>Kafka 消费者</h2><h3 id="Kafka消费方式"><a href="#Kafka消费方式" class="headerlink" title="Kafka消费方式"></a>Kafka消费方式</h3><p><strong>pull 模式</strong>：consumer采用从broker中主动拉取数据。 Kafka采用这种方式。不足之处是，如果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。</p>
<p><strong>push 模式</strong>：Kafka没有采用这种方式，因为由broker 决定消息发送速率，很难适应所有消费者的消费速率。例如推送的速度是50m/s， Consumer1、Consumer2就来不及处理消息。 </p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706161522608.png" alt="image-20220706161522608"></p>
<h3 id="offset-偏移量"><a href="#offset-偏移量" class="headerlink" title="offset 偏移量"></a>offset 偏移量</h3><p>offse 偏移量有两种：</p>
<ol>
<li>生产者的offset，是<strong>分区最大的offset</strong></li>
<li><strong>消费者的offset</strong>，指出消费者消费到哪条消息</li>
</ol>
<p><strong>生产者offset：</strong></p>
<p>生产者消息会分配到特定的分区里，每个分区都有一个offset，也是最大的offset，只会加到末尾，不会删除</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706105326528.png" alt="image-20220706105326528"></p>
<p><strong>消费者offset：</strong></p>
<p>消费者C1 C2来自不同的消费者组（一个分区一个消费者组里的消费者只能消费一次）</p>
<p>生产者上次提交的偏移量是5，C1消费到了2，C2消费到了4，下次消费的时候，C1、C2就分别从上次消费的 offset 位置开始消费，当然也可以选择从头开始或者从最近的记录开始。</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706105717453.png" alt="image-20220706105717453"></p>
<p>当消费者在读取一个没有offset的分区或者offset无效的情况下，auto.offset.reset 配置指定了该如何处理</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认值，在offset无效的情况下，消费者从最新的记录开始读取数据（也就是消费者启动后生成的记录）也就对应默认的命令</span></span><br><span class="line">auto.offset.reset=latest</span><br><span class="line"><span class="meta">#</span><span class="bash"> 消费者从起始位置读取分区的记录 对应命令参数 --from-beginning</span></span><br><span class="line">auto.offset.reset=earliest</span><br><span class="line"><span class="meta">#</span><span class="bash"> 抛出异常</span></span><br><span class="line">auto.offset.reset=none</span><br></pre></td></tr></table></figure>



<p>从0.9版本开始，consumer默认将offset保存在Kafka 一个内置的topic中，该topic为**__consumer_offsets**。Kafka0.9版本之前， consumer默认将offset 保存在Zookeeper中<img src="/../../img/markdown_img/kafka.assets/image-20220706162335248.png" alt="image-20220706162335248"></p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706162233817.png" alt="image-20220706162233817"></p>
<p>__consumer_offsets 主题里面采用 <strong>key 和 value 的方式存储数据</strong>。<strong>key</strong> 是 <strong>group.id+topic+ 分区号</strong>，<strong>value</strong> 就是当前 <strong>offset</strong> 的值。每隔一段时间，kafka 内部会对这个 topic 进行 <strong>compact</strong>，也就是每个 group.id+topic+分区号就保留最新数据。</p>
<p><strong>__consumer_offsets</strong> 也是主题，也是可以消费的，需要添加配置  exclude.internal.topics=false，默认是true 表示不能消费系统主题，改为false即可消费系统主题</p>
<h3 id="自动提交-offset"><a href="#自动提交-offset" class="headerlink" title="自动提交 offset"></a>自动提交 offset</h3><p>Kafka提供了自动提交offset的功能。</p>
<p>自动提交offset的相关参数： </p>
<ul>
<li><strong>enable.auto.commit</strong>：是否开启自动提交offset功能，默认是true。消费者会自动周期性地向服务器提交偏移量。</li>
<li><strong>auto.commit.interval.ms</strong>：自动提交offset的时间间隔，默认是5s</li>
</ul>
<p><strong>自动提交 offset 的重复消费问题：</strong></p>
<p><strong>自动提交有一个时间间隔</strong>的，如果 这段时间间隔消费者出现了故障，offset 没有提交到分区的主题，那么分区主题的offset就没有被更新。就会导致消息再被消费一次，也就是出现 <strong>重复消费</strong></p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706172947910.png" alt="image-20220706172947910"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String topic = <span class="string">&quot;first_test_topic&quot;</span>;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置自动offset 提交</span></span><br><span class="line">    properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">// 提交间隔，默认5s</span></span><br><span class="line">    properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;test1&quot;</span>);</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册要消费的主题</span></span><br><span class="line">    ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    topics.add(topic);</span><br><span class="line">    <span class="comment">// 订阅</span></span><br><span class="line">    kafkaConsumer.subscribe(topics);</span><br><span class="line">    <span class="comment">// 拉取数据打印</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 设置1s 消费一批数据</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="comment">// 打印 消费到的数据</span></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="手动提交-offset"><a href="#手动提交-offset" class="headerlink" title="手动提交 offset"></a>手动提交 offset</h3><p>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因 此Kafka还提供了手动提交offset的API。 </p>
<p>手动提交offset的方法有两种：分别是 <strong>commitSync</strong>（同步提交）和 <strong>commitAsync</strong>（异步提交）。</p>
<p><strong>相同点</strong>：都会将本次提交的一批数据最高的偏移量提交；</p>
<p><strong>不同点</strong>：同步提交 <strong>阻塞当前线程</strong>，一<strong>直到提交成功</strong>，<strong>并且会自动失败重试</strong>（由不可控因素导致，也会出现提交失败）；而<strong>异步提交</strong> 则<strong>没有失败重试机制</strong>，故有<strong>可能提交失败</strong>。</p>
<ul>
<li><strong>commitSync</strong>（同步提交）：<strong>必须等待offset提交完毕</strong>，再去消费下一批数据。 </li>
<li><strong>commitAsync</strong>（异步提交） ：<strong>发送完提交offset请求后</strong>，就开始消费下一批数据了。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String topic = <span class="string">&quot;first_test_topic&quot;</span>;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置手动offset 提交</span></span><br><span class="line">    properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;test1&quot;</span>);</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册要消费的主题</span></span><br><span class="line">    ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    topics.add(topic);</span><br><span class="line">    <span class="comment">// 订阅</span></span><br><span class="line">    kafkaConsumer.subscribe(topics);</span><br><span class="line">    <span class="comment">// 拉取数据打印</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 设置1s 消费一批数据</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="comment">// 打印 消费到的数据</span></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 异步提交offset</span></span><br><span class="line">        kafkaConsumer.commitAsync();</span><br><span class="line">        <span class="comment">// 同步提交</span></span><br><span class="line">        <span class="comment">//kafkaConsumer.commitSync();</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>由于同步提交 offset 有失败重试机制，故更加可靠，但是由于一直等待提交结果，提交的效率比较低。</p>
<p>虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。异步提交 offset 的方式效率高。</p>
<h3 id="指定-offset-消费"><a href="#指定-offset-消费" class="headerlink" title="指定 offset 消费"></a>指定 offset 消费</h3><p><strong>auto.offset.reset = earliest | latest | none</strong> 默认是 latest。 </p>
<p>当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量 时（例如该数据已被删除）</p>
<ol>
<li>earliest：自动将偏移量重置为最早的偏移量，–from-beginning</li>
<li>latest（默认值）：自动将偏移量重置为最新偏移量，就是 消费consumer启动后 生产的消息</li>
<li>none：如果未找到消费者组的先前偏移量，则向消费者抛出异常</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String topic = <span class="string">&quot;first_test_topic&quot;</span>;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须 配置消费者组</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;test2&quot;</span>);</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册要消费的主题</span></span><br><span class="line">    ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    topics.add(topic);</span><br><span class="line">    <span class="comment">// 订阅</span></span><br><span class="line">    kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line">    <span class="comment">// 保证 consumer lead 把分区分配方案制定完毕</span></span><br><span class="line">    <span class="keyword">while</span> (assignment.size()==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 没有获取到的话分区分配方案的话 我们之间poll 帮他拉取一下</span></span><br><span class="line">        kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="comment">//获取消费者分区分配信息 有了分区分配信息才能消费</span></span><br><span class="line">        assignment = kafkaConsumer.assignment();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 遍历所有分区，并指定从 offset 100的位置开始消费</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">        kafkaConsumer.seek(topicPartition,<span class="number">100</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//消费该主题</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 设置1s 消费一批数据</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="comment">// 打印 消费到的数据</span></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>







<h3 id="Kafka消费者工作流程"><a href="#Kafka消费者工作流程" class="headerlink" title="Kafka消费者工作流程"></a>Kafka消费者工作流程</h3><p><img src="/../../img/markdown_img/kafka.assets/image-20220706162155877.png" alt="image-20220706162155877"></p>
<h3 id="消费者组原理"><a href="#消费者组原理" class="headerlink" title="消费者组原理"></a>消费者组原理</h3><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。</p>
<ul>
<li>消费者组内<strong>每个消费者负责消费不同分区的数据</strong>，一个分区只能由一个组内消费者消费。 </li>
<li><strong>消费者组之间互不影响</strong>。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
</ul>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706162518256.png" alt="image-20220706162518256"></p>
<ol>
<li>如果向消费组中添加更多的消费者，<strong>消费者组内的消费者数 超过 主题分区数量</strong>，则有一<strong>部分消费者</strong>就会<strong>闲置</strong>，不会接收任何消息</li>
<li>消费者组之间互不影响</li>
</ol>
<p><strong>消费者组初始化流程：</strong></p>
<p><strong>coordinator</strong>：辅助实现消费者组的初始化和分区的分配。</p>
<p> coordinator节点选择 = groupid的hashcode值 % 50 (<strong>consumer_offsets的分区数量为50</strong>）</p>
<p>groupid 是客户端 手动设置的一个groupid</p>
<p>__例如： groupid的hashcode值 = 1，1% 50 = 1，那么__consumer_offsets 主题的1号分区，在哪个broker上，就选择这个节点的coordinator 作为这个消费者组的老大。消费者组下的所有的消费者提交offset的时候就往这个分区去提交offset。</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706162752859.png" alt="image-20220706162752859"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zc.kafkademo.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个独立消费者，消费 指定 主题中数据。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> String topic = <span class="string">&quot;first_test_topic&quot;</span>;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 必须 配置消费者组</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;test1&quot;</span>);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topics.add(topic);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        // 消费某个主题的某个分区数据</span></span><br><span class="line"><span class="comment">        ArrayList&lt;TopicPartition&gt; topicPartitions = new</span></span><br><span class="line"><span class="comment">                ArrayList&lt;&gt;();</span></span><br><span class="line"><span class="comment">        topicPartitions.add(new TopicPartition(topic, 0));</span></span><br><span class="line"><span class="comment">        kafkaConsumer.assign(topicPartitions);</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 订阅</span></span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 设置1s 消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 打印 消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;</span><br><span class="line">                System.out.println(record);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="消费者分区的分配"><a href="#消费者分区的分配" class="headerlink" title="消费者分区的分配"></a>消费者分区的分配</h3><ol>
<li><p>一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个 partition的数据。 </p>
</li>
<li><p>Kafka有四种主流的分区分配策略： <strong>Range</strong>、<strong>RoundRobin</strong>、<strong>Sticky</strong>、<strong>CooperativeSticky</strong>。 可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用 多个分区分配策略。</p>
</li>
</ol>
<p>Kafka默认的消费逻辑是，位于某个主题中的一个分区只能被同一个消费者组中的一个消费者消费。<br>当主题分区发生变化时、或有新消费者加入群组时、或群组中有消费者挂掉时，Kafka会触发分区再均衡操作。 那分区再均衡有哪些优缺点呢？分区再平衡的优势在于能为消费者群组带来高可用性与伸缩性；但其缺点在于在发生再均衡这一期间内，消费者是无法读取信息的，所以这将会造成消费者群组会出现一小段时间不可用的情形。所以在应用Kafka的过程中，需要避免无用的分区再平衡发生。</p>
<ol>
<li><p>RoundRobin</p>
<p>RoundRobin 就是轮询，比如 在消费者订阅的主题都一致的情况下：假设有 3 个 topic，10 个 partition，分配结果就如下</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220707154635300.png" alt="image-20220707154635300"></p>
<p>如果 消费者订阅的主题不一致，那么结果会是这样：</p>
<p>C0轮询到 Topic-A-Partition-0，到C1了，C1只能选择Topic-B-Partition-0；</p>
<p>C0轮询到 Topic-A-Partition-1，到C1了，C1只能选择Topic-B-Partition-1；</p>
<p>…</p>
<p>最后 Topic-C 的所有分区都只能 C1 消费</p>
<p>所以，最好不要选择 RoundRobin</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220707154847183.png" alt="image-20220707154847183"></p>
</li>
<li><p>Range</p>
<p>Range 是默认的策略，不同于RoundRobin之处在于<strong>Range策略</strong>是<strong>面向Topic分配</strong>的(RoundRobin面向Partition)</p>
<p>首先对同一个 topic 里面的分区按照序号进行排序，并 对消费者按照字母顺序进行排序。</p>
<p>假如现在有 7 个分区，3 个消费者，排序后的分区将会 是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。</p>
<p>例如，7/3 = 2 余 1 ，除不尽，那么 消费者 C0 便会多 消费 1 个分区。 8/3=2余2，除不尽，那么C0和C1分别多 消费一个。 通过 partitions数/consumer数 来决定每个消费者应该 消费几个分区。如果除不尽，那么前面几个消费者将会多 消费 1 个分区。 分区分配策略之Range</p>
<p>注意：如果<strong>只是针对 1 个 topic 而言</strong>，<strong>C0</strong>消费者<strong>多消费1 个</strong>分区影响不是很大。但是如果<strong>有 N 多个 topic</strong>，那么针对每 个 topic，消费者 C0都将多消费 1 个分区，topic越多，<strong>C0消 费的分区</strong>会比其他消费者明显<strong>多消费 N 个分区</strong>。 容易产生数据倾斜！</p>
<ol start="3">
<li><p>Sticky</p>
<p>可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前， <strong>考虑上一次分配的结果</strong>，尽量少的调整分配的变动，可以节省大量的开销。 粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区 到消费者上面，在出现同一消费者组内消费者出现问题的时候，会<strong>尽量保持原有分配的分 区不变化</strong>。</p>
<p>对于同一个主题，7个分区，3个消费者，那么就是 7/3=2…1，所以就3个消费者，C1 消费3个分区，C2和C3都消费两个。虽然这个结果消费数量看似和Range一样，但是Range里消费的分区都是排序了的，比如C1 消费的3个分区就应该是 0 1 6。而Sticky则是随机的分配到消费里面去，序号什么的无所谓，只有数量能对上</p>
<p>对于订阅不同主题：</p>
<p>假设C1订阅的T0，C2订阅T0，T1，C3订阅T0，T1，T2 对于RoundRobin，</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220707161140326.png" alt="image-20220707161140326"></p>
<p>这时候 假设 C1脱离了群组，根据RoundRobin再分配就是这样</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220707161226081.png" alt="image-20220707161226081"></p>
<p>对于Sticky，最终分配结果为</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220707161416151.png" alt="image-20220707161416151"></p>
<p>C1脱离，平衡结果为</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220707161438240.png" alt="image-20220707161438240"></p>
</li>
</ol>
</li>
</ol>
<h2 id="Kafka-幂等性-和-事务"><a href="#Kafka-幂等性-和-事务" class="headerlink" title="Kafka 幂等性 和 事务"></a>Kafka 幂等性 和 事务</h2><h3 id="Kafka-为啥需要幂等性"><a href="#Kafka-为啥需要幂等性" class="headerlink" title="Kafka 为啥需要幂等性"></a>Kafka 为啥需要幂等性</h3><p>Kafka在引入幂等性之前，Producer向Broker发送消息，然后Broker将消息追加到消息流中后给Producer返回Ack信号值。可能存在很多不确定的因素：比如 Producer 给 Broker 发送了信息， Broker 接收到了消息，在返回 ack 的时候网络异常等，Producer 没有收到。Producer 就触发重试机制，就会重新发送一条一样的消息给Broker，这条消息又被加到了某个partition</p>
<h3 id="幂等性"><a href="#幂等性" class="headerlink" title="幂等性"></a>幂等性</h3><p><strong>幂等性</strong>就是指Producer不论向Broker发送多少次重复数据，Broker端都<strong>只会持久化一条</strong>，<strong>保证了不重复</strong>。 </p>
<p><strong>幂等性</strong>只在<strong>单个partition内</strong>能保证，<strong>跨partition的幂等性</strong>不能保证。要保证不同partition和topic之间的幂等性，则需要 <strong>考虑 Kafka 事务</strong></p>
<p>Kafka 为了实现幂等性，解决Producer重试引起的重复。Kafka 增加了 Pid 和 Seq。</p>
<ul>
<li><p><strong>PID</strong>：每一个 Producer都分配一个唯一的PID。Producer 故障后重新启动后会被分配一个新的 PID。Producer 的 PID 向 server 端申请，server 端Controller 又会向 Zookeeper 申请，Zookeeper 可以生成分布式唯一ID，ZK 中有一个 /last_producer_id_block 节点，每申请一个 PID 段，都会把自己申请的PID段信息写入到/last_producer_id_block 节点，当下次 又有broker申请的PID的时候先读取这个节点的信息，然后根据block_end选择一个PID段，最后再把信息写回到ZK节点。broker向producer返回自己保存的PID。</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706143045048.png" alt="image-20220706143045048"></p>
</li>
<li><p>Sequence Number：Producer 发送数据的每个 topic partition 都对应一个从 0 单调递增的SequenceNumber。</p>
</li>
</ul>
<p>这样，Producer端会保存每一个，如果Producer 重试 再次发送消息，会携带PID 和 Seq ，在Boker那边能发现之前有这么一条消息，就不重复。</p>
<p><strong>不同的分区它的 序列化 Seq 是不同的，这就是为什么这个幂等性只在单个partition内能保证</strong></p>
<p><strong>使用幂等性</strong>：<strong>开启参数 enable.idempotence 默认为 true</strong></p>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>Kafka中的事务与数据库的事务类似，Kafka中的事务属性是指一系列的Producer生产消息和消费消息提交Offsets的操作在一个事务中，即原子性操作。对应的结果是同时成功或者同时失败。<strong>kafka中的事务可以使应用程序将消费消息，生产消息、提交消费位移当作原子操作来处理，同时成功或者失败，即使该生产或消费跨越多个分区。</strong></p>
<p>kafka事务引入了 <strong>transactionId</strong> 和Epoch，设置<strong>transactional.id</strong>后，<strong>一个transactionId只对应一个pid</strong>, 且 Server 端会记录最新的 Epoch 值（<strong>Epoch是分配给Producer的一个递增的值</strong>）。这样<strong>有新的producer初始化</strong>时，会向<strong>TransactionCoordinator</strong>发送<strong>InitPIDRequest</strong> <strong>请求PID</strong>， TransactionCoordinator 已经有了这个 transactionId对应的 信息，<strong>会返回之前分配的 PID</strong>，<strong>并把 Epoch 自增 1 返回</strong>，这样<strong>当 old producer 恢复</strong>过来请求操作时，检查Epoch值不是最新的，将<strong>被认为是无效producer ** 抛出异常。如果</strong>没有开启事务<strong>，</strong>TransactionCoordinator会为新的producer返回new pid**，这样就起不到隔离效果，因此无法实现多会话幂等。</p>
<p>原来的幂等性是分配一个PID，然后跟每个topic和每个分区都有关，这里可以理解为就用一个整体的id 也就是 transaction.id 来代替了幂等性的那一串PID，Sequence等，所以跟分区无关了。也就实现了跨分区跨主题</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706160033630.png" alt="image-20220706160033630"></p>
<p><strong>Transaction Log：</strong>同 消费者offset <strong>__consumer_offsets</strong>一样，是一个系统topic  <strong>__transaction_state</strong> <strong>存储</strong>一些<strong>事务的最新状态</strong> 和 一些相关<strong>元数据信息</strong>。每个 transactional.id 通过 hash 都对应到 了 Transaction Log 的一个分区，所以每个 transactional.id 都有且仅有一个 transaction coordinator 负责。</p>
<p><strong>控制消息：</strong>专门用来标记事务的状态，同其他的正常消息一样都被存在日志中，但是 控制消息 不会返回给 consumer。控制消息共有两种类型：commit 和 abort，分别用来表征事务已经成功提交或已经被成功终止</p>
<p><strong>整个具体的事务流程：</strong></p>
<ol>
<li><p><strong>查找Transaction Coordinator</strong></p>
<p><strong>Producer</strong> 向任意一个Broker 发送 <strong>FindCoordinatorRequest</strong> 请求，<strong>获取到 Transaction Coordinator 的地址</strong></p>
</li>
<li><p><strong>初始化事务 initTransaction</strong></p>
<p>Producer 发送 <strong>initPidRequest</strong> 给 Transaction Coordinator，<strong>获取 PID</strong>， Transaction Coordinator 在 Transaction log 中<strong>建立 &lt; TransactionId,pid &gt;的映射</strong></p>
</li>
<li><p><strong>开始事务 beginTransaction</strong></p>
<p>执行Producer的beginTransacion()，它的作用是 <strong>Producer 在本地记录下这个transaction的状态为开始状态</strong>。这个操作并没有通知Transaction Coordinator，因为Transaction Coordinator只有在Producer发送第一条消息后才认为事务已经开启</p>
</li>
<li><p><strong>消息发送与记录</strong></p>
<p>一旦 Producer 开始发送消息， Transaction Coordinator 会将该 &lt;Transaction,Topic,Partition&gt; 存于 <strong>Transaction Log</strong> 内，设置事务状态开始。此外，针对该事务中第一个消息，Transaction Coordinator 在处理之前会开启计时，每个事务都有自己的超时时间</p>
<p>在注册 &lt;Transaction,Topic,Partition&gt; 于 Transaction Log 内，生产者发送数据，尽管没有被 Commit 或者 Abort，此时消息已经被持久化。即使后面Abort，也只是把消息字段标识修改为Abort标识</p>
</li>
<li><p><strong>事务提交或者丢弃  commitTransaction/abortTransaction</strong></p>
<p>Producer 在 提交或丢弃 的时候，Transaction Coordinator 也会有一个 <strong>两阶段提交</strong></p>
<ol>
<li>将 <strong>Transaction Log</strong> 内的<strong>事务状态</strong>改为 <strong>Prepare_Commit</strong> 或者 <strong>Prepare_Abort</strong></li>
<li>将 控制消息 写入该事务涉及到的所有消息（即将所有消息 标记为 Commit/Abort）。<strong>Transaction Coordinator 在向 topic-partition 的leader写完控制消息后，更新事务状态为 Commited 或者 Abort</strong></li>
</ol>
</li>
</ol>
<p>Kafka 事务下的API</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1 初始化事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initTransactions</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 2 开启事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params"> String consumerGroupId)</span> <span class="keyword">throws</span> </span></span><br><span class="line"><span class="function">ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 4 提交事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 5 放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerTransactions</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">         <span class="comment">// 1. 创建 kafka 生产者的配置对象</span></span><br><span class="line">         Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">// 2. 给 kafka 配置对象添加配置信息</span></span><br><span class="line">         properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.73.137:9092&quot;</span>);</span><br><span class="line">         <span class="comment">// key,value 序列化</span></span><br><span class="line">         properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, </span><br><span class="line">                        StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">         properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, </span><br><span class="line">                        StringSerializer.class.getName());</span><br><span class="line">         <span class="comment">// 必须设置事务 id，事务 id 任意起名</span></span><br><span class="line">         properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, </span><br><span class="line">                        <span class="string">&quot;transaction_id_0&quot;</span>);</span><br><span class="line">         <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">         KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> </span><br><span class="line">             KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line">         <span class="comment">// 初始化事务</span></span><br><span class="line">         kafkaProducer.initTransactions();</span><br><span class="line">         <span class="comment">// 开启事务</span></span><br><span class="line">         kafkaProducer.beginTransaction();</span><br><span class="line">         <span class="keyword">try</span> &#123;</span><br><span class="line">             <span class="comment">// 4. 调用 send 方法,发送消息</span></span><br><span class="line">             <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                 <span class="comment">// 发送消息</span></span><br><span class="line">                 kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first_test_topic&quot;</span>, </span><br><span class="line">                                                         <span class="string">&quot;ins1mnia &quot;</span> + i));</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="comment">// int i = 1 / 0;这样就不会提交成功，一条消息都发送不出去</span></span><br><span class="line">             <span class="comment">// 提交事务</span></span><br><span class="line">             kafkaProducer.commitTransaction();</span><br><span class="line">         &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">             <span class="comment">// 终止事务</span></span><br><span class="line">             kafkaProducer.abortTransaction();</span><br><span class="line">         &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">             <span class="comment">// 5. 关闭资源</span></span><br><span class="line">             kafkaProducer.close();</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="数据同步原理"><a href="#数据同步原理" class="headerlink" title="数据同步原理"></a>数据同步原理</h2><h3 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h3><p>Kafka每个topic可以分为多个Partition，并且 <strong>多个Partition 会均匀的分布在集群的各个节点下</strong>。这样能够有效的对数据进行分片。但是 单点的Partition并不可靠，所以 Kafka 提出了<strong>副本（Replica）</strong>。通过 副本机制来实现冗余备份。</p>
<p>每个分区可以有多个副本。副本中 有一个 Leader 和 多个 Follower。所有的读写请求都由 Leader 进行处理，Follower 需要和 Leader 同步消息日志。</p>
<p>一般情况下，同一个分区的多个副本会被均匀分配到集群中的不同 broker上，这样当 某个 分区的Leader所在的 broker挂了，不会出现 所有follower也挂了。可以从其他 broker的这个分区的副本 选举出一个新的 Leader。</p>
<h3 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h3><p><strong>ISR（In-Sync Replicas）：</strong>ISR 包含了 所有与Leader完全同步的Follower集合，可能有部分Follower故障没有与Leader完全同步，那么这个Follower就不在ISR集合中，而在 <strong>OSR（Out-Sync Relipcas）</strong>集合中，两个集合的并集，就是 所有副本 <strong>AR（Assigned Repllicas）</strong></p>
<p><strong>LEO（Log end offset）：</strong>日志末端位置，记录了该副本日志中<strong>下一条消息</strong>的位移量。下一条消息 表示 LEO=10，那么 该副本保存了10条消息，位移值范围是 [0,9]。Leader LEO 和 Follower LEO 的更新有所区别</p>
<p><strong>HW（High Watermark）：</strong>高水位值，对于同一个副本来说，HW &lt;= LEO。小于等于 HW 值的所有消息都认为是 <strong>已经备份</strong> 的。</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706113543512.png" alt="image-20220706113543512"></p>
<p><strong>具体流程：</strong></p>
<ol>
<li>broker1上的 leader 副本接收到消息，把自己的 LEO 值更新为 1 。</li>
<li>broker2 和 broker3 上的 follower 副本各自发送请求给 broker 1。（一般情况下，是follower定时给leader发送fetch请求的，就好像heartbeat心跳）</li>
<li>broker1收到fetch请求后，主动分别把该消息推送给 follower 副本 。</li>
<li>follower 副本接收到消息后各自更新自己的 LEO 为 1，并返回response。</li>
<li>leader 副本接收到其他 follower 副本的数据请求响应（ response ）之后，更新 HW 值为 1 。 此时位移为 0 的这条消息可以被 consumer 消费。</li>
</ol>
<p><strong>Leader选举过程：</strong></p>
<ol>
<li><strong>Kafka Controller</strong>监听 Zookeeper 的 /brokers/ids 下的所有节点，一旦有 broker 挂了，就进行Leader的选举</li>
<li>优先从 ISR 列表中选出第一个作为leader副本，具体过程：<strong>按照 AR 集合中副本的顺序 查找 第一个 存活的，并且 属于 ISR 集合的 副本作为新的 leader</strong>。一个分区的AR集合在创建分区副本的时候就被指定，只要不发生重分配的情况，AR集合内部副本的顺序是保持不变的，而分区的ISR集合上面说过因为同步滞后等原因可能会改变，所以注意这里是根据AR的顺序而不是ISR的顺序找。</li>
<li>一种 <strong>极端情况</strong>：<strong>所有的partition副本都不可用</strong>。Kafka 提供了两种选择<ul>
<li>选择 ISR 中 第一个 活 过来的副本作为 Leader</li>
<li>选择 第一个活过来的 副本（不一定是ISR中的）作为 Leader</li>
<li>这就需要在<strong>可用性</strong>和<strong>数据一致性</strong>当中做出选择，如果一定要<strong>等待ISR中的副本</strong>活过来，那<strong>不可用</strong>的时间可能会相对<strong>较长</strong>。选择<strong>第一个活过来的副本</strong>作为Leader，如果这个副本不在ISR中，那<strong>数据的一致性</strong>则难以<strong>保证</strong>。kafka<strong>支持用户通过配置选择</strong>，以根据业务场景在可用性和数据一致性之间做出权衡。</li>
</ul>
</li>
<li>Kafka Controller 也是有可能挂掉的，所以也有 Controller 的选举过程</li>
</ol>
<h2 id="Kafka-Controller"><a href="#Kafka-Controller" class="headerlink" title="Kafka Controller"></a>Kafka Controller</h2><p>Controller 控制器，是Kafka 集群中的核心组件，Kafka 集群中每一个Broker都可以是一个Controller，但是一个 Kafka 集群，同一时间 只能有一个 Controller 节点。</p>
<p>Controller 的作用 主要是 和 Zookeeper 协调管理 整个 Kafka集群。Controller 同 Zookeeper 进行交互，获取和更新集群中的元数据信息。这样，其他 Broker 就不用直接与 Zookeeper 进行通信，而是与 Controller 进行通信，并且同步Controller中的元数据信息。</p>
<p>元数据</p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706143036219.png" alt="image-20220706143036219"></p>
<p><img src="/../../img/markdown_img/kafka.assets/image-20220706143045048.png" alt="image-20220706143045048"></p>
<p><strong>Controller 主要作用</strong></p>
<ol>
<li><p><strong>主题管理</strong>：<br>创建、删除Topic，以及增加Topic分区等操作都是由控制器执行。</p>
</li>
<li><p><strong>分区重分配</strong>：<br>执行Kafka的reassign脚本对Topic分区重分配的操作，也是由控制器实现。<br>如果集群中有一个<strong>Broker异常退出</strong>，控制器会检查这个broker是否有分区的副本leader，如果有那么这个分区就需要一个新的leader，此时控制器就会去遍历其他副本，<strong>选举新leader</strong>，同时更新分区的ISR集合。<br>如果有一个<strong>Broker加入集群</strong>中，那么控制器就会通过Broker ID去判断新加入的Broker中是否含有现有分区的副本，如果有，就会从分区副本中去同步数据。</p>
</li>
<li><p><strong>Preferred leader选举</strong>：<br>因为在Kafka集群长时间运行中，broker的宕机或崩溃是不可避免的，leader就会发生转移，即使broker重新回来，也不会是leader了。在众多leader的转移过程中，就会产生<strong>leader不均衡现象</strong>，可能一<strong>broker上有大量的leader</strong>，影响了整个集群的性能，所以就需要把leader调整回最初的broker上，这就需要Preferred leader选举。</p>
</li>
<li><p><strong>集群成员管理</strong>：<br>控制器能够<strong>监控新broker的增加</strong>，<strong>broker的主动关闭与被动宕机</strong>，进而做其他工作。这也是利用Zookeeper的ZNode模型和Watcher机制，控制器会监听Zookeeper中/brokers/ids下临时节点的变化。同时对broker中的leader节点进行调整。</p>
<p>比如，控制器组件会利用 Watch 机制检查 ZooKeeper 的 /brokers/ids 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 /brokers 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。</p>
<p>侦测 Broker 存活性则是依赖于刚刚提到的另一个机制：临时节点。每个 Broker 启动后，会在 /brokers/ids 下创建一个临时 znode。当 Broker 宕机或主动关闭后，该 Broker 与 ZooKeeper 的会话结束，这个 znode 会被自动删除。同理，ZooKeeper 的 Watch 机制将这一变更推送给控制器，这样控制器就能知道有 Broker 关闭或宕机了，从而进行“善后”</p>
</li>
<li><p><strong>元数据服务</strong>：<br>控制器上<strong>保存</strong>了<strong>最全的集群元数据信息</strong>，其他所有broker会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据</p>
</li>
</ol>
<h2 id="线上问题优化"><a href="#线上问题优化" class="headerlink" title="线上问题优化"></a>线上问题优化</h2><h3 id="如何防止消息丢失"><a href="#如何防止消息丢失" class="headerlink" title="如何防止消息丢失"></a>如何防止消息丢失</h3><p><strong>Producer 采用调用回调函数的异步发送：</strong></p>
<p>Producer端很可能会丢失消息，send(msg)这个API通常会立即返回，并不能保证消息发送成功。可能网络等原因，消息就没发送到Broker；或者消息本身不合规，比如消息太大，Broker拒收</p>
<p>**采用 send(msg，callback)**，带有回调函数的发送，一旦出现异常情况，就可以针对性的处理，利用回调函数，<code>Producer</code>重试就可以了；如果是消息不合规造成的，那么调整消息格式后再次发送。</p>
<p><strong>设置 acks 为 1 或者 -1/all：</strong></p>
<p>1 保证 分区 leader 把消息落入磁盘。</p>
<p>-1/all 要保证 ISR 中所有Follower 都把这条消息同步过后才返回。</p>
<p><strong>看消息的重要程度，可以设置为 1 或者 -1</strong></p>
<h3 id="Kafka如何保证消息的顺序消费"><a href="#Kafka如何保证消息的顺序消费" class="headerlink" title="Kafka如何保证消息的顺序消费"></a>Kafka如何保证消息的顺序消费</h3><p>如果我们用 Kafka 想要保证消息的有序性</p>
<p>Kafka 我们可以选择 Kafka 一个 topic，一个 partition，一个consumer，这样随便来条消息都是绝对有序的了。</p>
<p>此外，我们可以选择<strong>满足一个Partition 的消息消费有序</strong>。首先要在 发消息的时候 都进入到同一个 Partition，我们可以选择 生产者 直接指定对应的 Partition，或者 指定 Partition的 Key，有Key 的情况下，会对 Key 做一个 Hash 运算 然后同 partition 数取余 得到 Partition 值。这样到消费者来取这个Partition的数据都是有序的。但是如果 消费者有多个线程并发的处理消息，消息取出来后执行的顺序就会得不到保证。</p>
<p>这时候，我们可以写多个 队列，将我们想要保证的顺序key相同的放到同一个队列里去。每一个线程消费一个队列，这样消费到队列上就是有序的了。</p>
<p>但是上面这种情况下，还是有异常的可能性。本来在同一个分区下的消息来说是有序的，但是，Kafka 可能因为消费者宕机或者重启会进行消费者组的分区重新分配消费，就可能导致乱序的情况</p>
<h3 id="消息重试对顺序消息的影响"><a href="#消息重试对顺序消息的影响" class="headerlink" title="消息重试对顺序消息的影响"></a>消息重试对顺序消息的影响</h3><p>对于两条消息 1,2，正常情况下的消息发送顺序是A，然后B。但是如果在A发送失败的情况下，B发送成功，A由于重试机制在B发送完成之后重试发送成功。这时候本身顺序AB就变为了BA。</p>
<blockquote>
<p>The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).</p>
</blockquote>
<p>这个时候我们需要设置 max.in.flight.requests.per.connection=1，客户端在单个连接上能够发送的未响应请求个数为1。这样，未响应的请求个数为1的时候，后面的消息不会进行发送，保证后一条消息发送的时候前一条消息是发送成功的。因此，这样也会降低系统的吞吐量，但是却不会因此造成乱序的情况</p>
<p>新版本kafka设置enable.idempotence=true后能够动态调整max-in-flight-request。正常情况下max.in.flight.requests.per.connection大于1。当重试请求到来且时，batch 会根据 seq重新添加到队列的合适位置，并把max.in.flight.requests.per.connection设为1，这样它 前面的 batch序号都比它小，只有前面的都发完了，它才能发。</p>
<h3 id="Kafka-如何防止消息的重复消费"><a href="#Kafka-如何防止消息的重复消费" class="headerlink" title="Kafka 如何防止消息的重复消费"></a>Kafka 如何防止消息的重复消费</h3><p>防止消息不丢失，生产者会有重试机制，可能造成消息重复消费；自动提交offset 也可能造成重复消费。</p>
<p>我们可以把自动提交改为手动提交，再加上 消费消息时的<strong>幂等性保证</strong>，就可以解决消息重复消费的问题。</p>
<p><strong>比如：</strong></p>
<p><strong>提高单条消息的处理速度</strong>，例如对消息处理中比较耗时的步骤可通过异步的方式进行处理、利用多线程处理等。</p>
<p>可以针对消息生成md5然后保存到mysql或者redis里面，在处理消息之前先去mysql或者redis里面判断是否已经消费过。这个方案其实就是<strong>利用幂等性的思想。</strong></p>
<p>给消息增加一个版本号属性，每次消费消息前，比较当前消息的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等。</p>
<h3 id="解决消息积压问题"><a href="#解决消息积压问题" class="headerlink" title="解决消息积压问题"></a>解决消息积压问题</h3><p>消息积压会导致很多问题，比如磁盘被打满、生产端发消息导致kafka性能过慢，就容易出现服务雪崩，就需要有相应的手段：</p>
<ul>
<li>方案一：在一个消费者中启动多个线程，<strong>让多个线程同时消费</strong>。——提升一个消费者的消费能力（<strong>增加分区增加消费者</strong>）。</li>
<li>方案二：如果方案一还不够的话，这个时候可以启动多个消费者，多个消费者部署在不同的服务器上。其实多个消费者部署在同一服务器上也可以提高消费能力——充分利用服务器的cpu资源。</li>
<li>方案三：让一个<strong>消费者去把收到的消息往另外一个topic上发</strong>，<strong>另一个topic设置多个分区和多个消费者 ，进行具体的业务消费</strong>。</li>
</ul>
<h3 id="延迟队列"><a href="#延迟队列" class="headerlink" title="延迟队列"></a>延迟队列</h3><p>延迟队列的应用场景：在订单创建成功后如果超过 30 分钟没有付款，则需要取消订单，此时可用延时队列来实现</p>
<ul>
<li>创建多个topic，每个topic表示延时的间隔<ul>
<li>topic_5s: 延时5s执行的队列</li>
<li>topic_1m: 延时 1 分钟执行的队列</li>
<li>topic_30m: 延时 30 分钟执行的队列</li>
</ul>
</li>
<li>消息发送者发送消息到相应的topic，并带上消息的发送时间</li>
<li>消费者订阅相应的topic，消费时轮询消费整个topic中的消息<ul>
<li>如果消息的发送时间，和消费的当前时间超过预设的值，比如 30 分钟</li>
<li>如果消息的发送时间，和消费的当前时间没有超过预设的值，则不消费当前的offset及之后的offset的所有消息都消费</li>
<li>下次继续消费该offset处的消息，判断时间是否已满足预设值</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">ins1mnia</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://ins1mn1a.github.io/2022/07/27/Kafka/Kafka/">http://ins1mn1a.github.io/2022/07/27/Kafka/Kafka/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://ins1mn1a.github.io" target="_blank">ins1mn1a</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Kafka/">Kafka</a></div><div class="post_share"><div class="social-share" data-image="/img/background_img/1/6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/27/ElasticSearch/ElasticSearch/"><img class="prev-cover" src="/img/background_img/1/2.jpg" onerror="onerror=null;src='/img/background_img/error_page.jpeg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Elasticsearch</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/27/SpringSecurity/SpringSecurity%20Oauth2/"><img class="next-cover" src="/img/background_img/1/4.jpg" onerror="onerror=null;src='/img/background_img/error_page.jpeg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SpringSecurity Oauth2</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://tvax1.sinaimg.cn/crop.0.0.996.996.180/007RWaeLly8gr7p82e9e6j30ro0rognd.jpg?KID=imgbed,tva&amp;Expires=1639625304&amp;ssig=yn%2FY%2BCtxLO" onerror="this.onerror=null;this.src='/img/background_img/error_page.jpeg'" alt="avatar"/></div><div class="author-info__name">ins1mnia</div><div class="author-info__description">because it feels like ins1mnia</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ins1mnia"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ins1mnia" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:ins1mnia@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">穿梭时间的画面的钟,从反方向开始移动</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">Kafka介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.1.</span> <span class="toc-text">Kafka使用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.</span> <span class="toc-text">Kafka基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">1.3.</span> <span class="toc-text">Kafka架构图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">Kafka基本使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E6%90%AD%E5%BB%BA"><span class="toc-number">2.1.</span> <span class="toc-text">Kafka搭建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">2.2.</span> <span class="toc-text">配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-topic"><span class="toc-number">2.3.</span> <span class="toc-text">创建 topic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF"><span class="toc-number">2.4.</span> <span class="toc-text">发送消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF"><span class="toc-number">2.5.</span> <span class="toc-text">消费消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E6%92%AD%E6%B6%88%E6%81%AF"><span class="toc-number">2.6.</span> <span class="toc-text">单播消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%92%AD%E6%B6%88%E6%81%AF"><span class="toc-number">2.7.</span> <span class="toc-text">多播消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%B6%88%E8%B4%B9%E7%BB%84"><span class="toc-number">2.8.</span> <span class="toc-text">查看消费组</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">3.</span> <span class="toc-text">Kafka集群搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">4.</span> <span class="toc-text">Kafka生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%8E%9F%E7%90%86"><span class="toc-number">4.1.</span> <span class="toc-text">生产者原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81"><span class="toc-number">4.2.</span> <span class="toc-text">生产者消息发送</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">4.2.1.</span> <span class="toc-text">分区策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%99%AE%E9%80%9A%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">4.2.2.</span> <span class="toc-text">普通异步发送</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%A6%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E7%9A%84%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">4.2.3.</span> <span class="toc-text">带回调函数的异步发送</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="toc-number">4.2.4.</span> <span class="toc-text">同步发送API</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">5.</span> <span class="toc-text">Kafka 消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E6%96%B9%E5%BC%8F"><span class="toc-number">5.1.</span> <span class="toc-text">Kafka消费方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#offset-%E5%81%8F%E7%A7%BB%E9%87%8F"><span class="toc-number">5.2.</span> <span class="toc-text">offset 偏移量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4-offset"><span class="toc-number">5.3.</span> <span class="toc-text">自动提交 offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8F%90%E4%BA%A4-offset"><span class="toc-number">5.4.</span> <span class="toc-text">手动提交 offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A-offset-%E6%B6%88%E8%B4%B9"><span class="toc-number">5.5.</span> <span class="toc-text">指定 offset 消费</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">5.6.</span> <span class="toc-text">Kafka消费者工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%8E%9F%E7%90%86"><span class="toc-number">5.7.</span> <span class="toc-text">消费者组原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D"><span class="toc-number">5.8.</span> <span class="toc-text">消费者分区的分配</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E5%B9%82%E7%AD%89%E6%80%A7-%E5%92%8C-%E4%BA%8B%E5%8A%A1"><span class="toc-number">6.</span> <span class="toc-text">Kafka 幂等性 和 事务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-%E4%B8%BA%E5%95%A5%E9%9C%80%E8%A6%81%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">6.1.</span> <span class="toc-text">Kafka 为啥需要幂等性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">6.2.</span> <span class="toc-text">幂等性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8B%E5%8A%A1"><span class="toc-number">6.3.</span> <span class="toc-text">事务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86"><span class="toc-number">7.</span> <span class="toc-text">数据同步原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC"><span class="toc-number">7.1.</span> <span class="toc-text">副本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Leader%E9%80%89%E4%B8%BE"><span class="toc-number">7.2.</span> <span class="toc-text">Leader选举</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-Controller"><span class="toc-number">8.</span> <span class="toc-text">Kafka Controller</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98%E4%BC%98%E5%8C%96"><span class="toc-number">9.</span> <span class="toc-text">线上问题优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1"><span class="toc-number">9.1.</span> <span class="toc-text">如何防止消息丢失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9"><span class="toc-number">9.2.</span> <span class="toc-text">Kafka如何保证消息的顺序消费</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%87%8D%E8%AF%95%E5%AF%B9%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">9.3.</span> <span class="toc-text">消息重试对顺序消息的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E6%B6%88%E6%81%AF%E7%9A%84%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">9.4.</span> <span class="toc-text">Kafka 如何防止消息的重复消费</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B%E9%97%AE%E9%A2%98"><span class="toc-number">9.5.</span> <span class="toc-text">解决消息积压问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97"><span class="toc-number">9.6.</span> <span class="toc-text">延迟队列</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/07/27/%E7%AE%97%E6%B3%95/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" title="滑动窗口"><img src="/img/background_img/1/6.jpg" onerror="this.onerror=null;this.src='/img/background_img/error_page.jpeg'" alt="滑动窗口"/></a><div class="content"><a class="title" href="/2022/07/27/%E7%AE%97%E6%B3%95/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" title="滑动窗口">滑动窗口</a><time datetime="2022-07-27T14:57:22.561Z" title="发表于 2022-07-27 22:57:22">2022-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/27/ElasticSearch/ElasticSearch/" title="Elasticsearch"><img src="/img/background_img/1/2.jpg" onerror="this.onerror=null;this.src='/img/background_img/error_page.jpeg'" alt="Elasticsearch"/></a><div class="content"><a class="title" href="/2022/07/27/ElasticSearch/ElasticSearch/" title="Elasticsearch">Elasticsearch</a><time datetime="2022-07-27T14:48:59.392Z" title="发表于 2022-07-27 22:48:59">2022-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/27/Kafka/Kafka/" title="Kafka"><img src="/img/background_img/1/6.jpg" onerror="this.onerror=null;this.src='/img/background_img/error_page.jpeg'" alt="Kafka"/></a><div class="content"><a class="title" href="/2022/07/27/Kafka/Kafka/" title="Kafka">Kafka</a><time datetime="2022-07-27T14:46:17.266Z" title="发表于 2022-07-27 22:46:17">2022-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/27/SpringSecurity/SpringSecurity%20Oauth2/" title="SpringSecurity Oauth2"><img src="/img/background_img/1/4.jpg" onerror="this.onerror=null;this.src='/img/background_img/error_page.jpeg'" alt="SpringSecurity Oauth2"/></a><div class="content"><a class="title" href="/2022/07/27/SpringSecurity/SpringSecurity%20Oauth2/" title="SpringSecurity Oauth2">SpringSecurity Oauth2</a><time datetime="2022-07-27T14:44:17.161Z" title="发表于 2022-07-27 22:44:17">2022-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/17/Java%E5%9F%BA%E7%A1%80/Java%E5%9F%BA%E7%A1%80/" title="Java总结"><img src="/img/background_img/1/8.jpg" onerror="this.onerror=null;this.src='/img/background_img/error_page.jpeg'" alt="Java总结"/></a><div class="content"><a class="title" href="/2022/05/17/Java%E5%9F%BA%E7%A1%80/Java%E5%9F%BA%E7%A1%80/" title="Java总结">Java总结</a><time datetime="2022-05-17T02:47:08.163Z" title="发表于 2022-05-17 10:47:08">2022-05-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By ins1mnia</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">it feels like ins1mnia</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://ins1mn1a.github.io/2022/07/27/Kafka/Kafka/'
    this.page.identifier = '2022/07/27/Kafka/Kafka/'
    this.page.title = 'Kafka'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Valine' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="you,are,you,are,my,favorite,medicine" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>